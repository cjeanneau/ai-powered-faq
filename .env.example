# Configuration du projet FAQ Intelligent
# Copier ce fichier vers .env et remplir les valeurs

# =============================================================================
# HUGGINGFACE
# =============================================================================
# Token HuggingFace (obtenir sur https://huggingface.co/settings/tokens)
# Nécessaire pour l'API Inference (LLM et certains modèles)
HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Modèle LLM pour la stratégie A et B (RAG)
# Alternatives: meta-llama/Llama-2-7b-chat-hf, google/flan-t5-large
LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# Modèle d'embeddings pour la recherche sémantique
# Léger et efficace, fonctionne en local
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Modèle Q&A extractif pour la stratégie C
# Alternative française: etalab-ia/camembert-base-squadFR-fquad-piaf
QA_MODEL=deepset/roberta-base-squad2

# =============================================================================
# PARAMÈTRES APPLICATIFS
# =============================================================================
# Seuil de confiance minimal pour retourner une réponse
CONFIDENCE_THRESHOLD=0.5

# Nombre de FAQ à récupérer pour le contexte (RAG et Q&A)
TOP_K_RESULTS=3

# Niveau de logging (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# API (pour les tests avec FastAPI)
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000