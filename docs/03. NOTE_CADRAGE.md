# Note de Cadrage - Projet FAQ Intelligent

**Étudiant(s)** : Cyril Jeanneau

**Date** : 12/01/2026

**Version** : 1.0

---

## 1. Contexte et objectifs

### 1.1 Contexte du projet

La Communauté de Communes Val de Loire Numérique souhaite moderniser son service d'accueil citoyen. Actuellement, les agents consacrent 60% de leur temps à répondre à des questions récurrentes sur les démarches administratives. L'objectif est de déployer un assistant intelligent (chatbot)capable d'automatiser ces réponses via une API intégrée au site web de la collectivité, permettant ainsi aux agents de se concentrer sur des tâches à plus forte valeur ajoutée et éviter le stress mental engendré par la répétitivité des réponses.

### 1.2 Objectifs du projet

**Objectif principal** :
Concevoir, développer et déployer une API d'assistance FAQ intégrant un modèle d'IA (LLM ou autres), en s'appuyant sur une démarche de benchmark rigoureuse pour identifier et implémenter la meilleure stratégie technique de réponse.

**Objectifs secondaires** :
- [Benchmark & Recommendation] Comparer objectivement 3 stratégies (LLM seul, RAG, Q&A extractif) via un benchmark et formuler une recommandation technique argumentée basée sur des métriques (exactitude, latence, hallucinations, etc.).
- [Implémentation & Industrialisation] Implémenter l'API avec la stratégie retenue et mettre en place une chaîne d'industrialisation (Tests, CI/CD).
- [Documentation] Produire une documentation technique, une documentation utilisateur, une documentation de l'API et un code source commenté.  

### 1.3 Périmètre

**Dans le périmètre** :
- Développement d'une API REST avec FastAPI.
- Benchmark de 3 stratégies d'IA : LLM seul, Recherche sémantique + LLM (RAG simplifié), Q&A extractif.
- Base de données FAQ fournie.
- Base de Q/R attendues fournie.
- Utilisation de composants open source (HuggingFace Inference API, Sentence-Transformers, ChromaDB/NumPy).
- Mise en place de tests automatisés(pytest) et d'un pipeline CI/CD (GitHub Actions).
- Conteneurisation via Docker.

**Hors périmètre** :
- Développement d'une interface frontend (ou peut-etre en bonus avec une POC sous Streamlit ou Gradio).
- Utilisation d'APIs payantes (OpenAI, Anthropic, etc.).
- Hébergement sur des bases de données cloud.
- Multi-langues

---

## 2. Compréhension des 3 stratégies

### 2.1 Stratégie A - LLM seul

**Principe** :
Le modèle de langage (LLM) est utilisé directement pour répondre à la question de l'utilisateur. Il s'appuie uniquement sur un prompt système définissant son rôle d'assistant et sur ses connaissances pré-entraînées (jusqu'à une date précise). Aucune information externe n'est fournie dynamiquement.

**Avantages attendus** :
- Simplicité maximale de mise en œuvre (architecture légère).
- Rapidité de réponse (pas d'étape de recherche intermédiaire).

**Inconvénients attendus** :
- Fort risque d'hallucinations sur des données spécifiques non connues du modèle.
- Connaissances obsolètes (limitées à la date de fin d'entraînement du modèle).
- Fort risque de reponses basés sur des faq d'autres administrations par exemple.

**Schéma simplifié** :
```
Question → [Prompt Système + LLM] → Réponse
```

### 2.2 Stratégie B - Recherche sémantique + LLM

**Principe** :
Cette stratégie (RAG simplifié) combine la recherche d'information et la génération. Une recherche sémantique (via embeddings) identifie d'abord les éléments de la FAQ les plus pertinents pour la question posée. Ces éléments sont ensuite injectés dans le prompt du LLM pour lui servir de contexte et guider sa réponse.

**Avantages attendus** :
- Réponses plus fiables et ancrées dans les données réelles de la collectivité.
- Réponses plus conviviales et naturelles (conversationnelles).
- Réduction des hallucinations par rapport au LLM seul.
- Possibilité de mettre à jour la base de connaissances sans réentraîner le modèle.

**Inconvénients attendus** :
- Complexité accrue (embeddings, indexation, moteur de recherche vectorielle).
- Dépendance forte à la qualité de la récupération des documents (retrieval).
- Latence plus importante dû au retrieval (recherche sémantique).

**Schéma simplifié** :
```
Question → [Embeddings + Recherche] → [Contexte en Prompt] → [LLM] → Réponse
```

### 2.3 Stratégie C - Q&A extractif

**Principe** :
Après une étape de recherche de documents pertinents, un modèle spécialisé dans le "Question-Answering" (type BERT/RoBERTa) est utilisé pour extraire le fragment de texte exact qui répond à la question au sein du contexte retrouvé, sans générer de nouvelle phrase.

**Avantages attendus** :
- Très grande précision sur des questions factuelles.
- Pas de risque d'invention de contenu (hallucination générative impossible).
- Modèles souvent plus rapides et légers que les LLM génératifs.

**Inconvénients attendus** :
- Moins convivial (réponses prédéfinies et souvent identiques).
- Ne peut synthétiser plusieurs informations.


**Schéma simplifié** :
```
Question → [Recherche Document] → [Modèle Extractif] → Réponse (Extrait)
```

---

## 3. Stack technique envisagée

### 3.1 Composants principaux

| Composant       | Technologie choisie   | Justification      |
|-----------------|-----------------------|--------------------|
| Langage         | Python 3.x            | Adapté pour IA     |
| Framework API   | FastAPI               | Rapide, Swagger    |
| LLM             | HuggingFace API       | Open source et gratuit, permet de tester facilement plusieurs modèles grace à l'ecosysteme HuggingFace|
| Embeddings      | Sentence Transformers | Open source et gratuit, permet de tester facilement plusieurs modèles|
| Tests           | Pytest                | Open source et gratuit, permet de tester le code et de s'assurer que tout fonctionne correctement|
| CI/CD           | GitHub Actions        | Open source et gratuit, permet de mettre en place un pipeline de CI/CD|
| Conteneurisation| Docker                | Open source et gratuit, permet de containeriser l'application et de la déployer facilement|

### 3.2 Modèles IA identifiés

| Usage | Modèle(s) | Source | Raison du choix |
|-------|--------|--------|-----------------|
| LLM (génération) | | HuggingFace | |
| Embeddings | | | |
| Q&A extractif | | | |

---

## 4. Planning prévisionnel

| Phase | Jours | Focus | Ressources prioritaires |
|-------|-------|-------|------------------------|
| Cadrage & Veille | J1-J2 | Comprendre RAG, embeddings, Q&A + Ptocole de benchmark| Jalon2 : Validation du protocole |
| Implémentation | J3-J5 | Coder les 3 stratégies | Jalon2 : démonstration des 3 stratégies |
| Benchmark | J6 | Résultat des benchmark | Rapport de benchmark |
| API | J7 | Développer l'API REST | Jalon3 : API validée |
| Tests & CI/CD & Doc & Monitoring| J8-J9 | Industrialiser | |
| Finalisation | J10 | Présentation + livrables |Finalisation Démonstration fonctionnelle |

---

## 5. Risques identifiés

| Risque | Probabilité | Impact | Mitigation |
|--------|-------------|--------|------------|
| API HuggingFace indisponible | Faible  | Service indisponible | Utiliser un fournisseur à haute disponibilité|
| Temps de latence trop long   | Moyenne | Insatisfaction client| |
| [Autre risque] | | | |

---

## 6. Questions en suspens

- [ ] [Question 1 pour le formateur]
- [ ] [Question 2]

---

## 7. Ressources consultées (Veille J1)

| Source                            | URL                                                                              | Pertinence | Notes |
|-----------------------------------|----------------------------------------------------------------------------------|------------|-------|
| AWS -  What is RAG?               | https://aws.amazon.com/what-is/retrieval-augmented-generation/                   |            |       |
| HuggingFace - Question Answering  | https://huggingface.co/tasks/question-answering                                  |            |       |


---
