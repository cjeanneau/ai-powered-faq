## 1. LLM 
source : https://aws.amazon.com/what-is/conversational-ai/
Expliques la différence entre IA conversationnelle et IA générative. 


## 2. RAG 

source :  https://aws.amazon.com/what-is/retrieval-augmented-generation/
Expliques ce qu'est un RAG, ses avantages pararpport a un LLM pur. 
On asscoie une base de connaissances avec un LLM pour obtenir des réponses plus précises. Cette base va etre vectorisée avec des embeddings pour permettre une recherche sémantique et en extraire les documents les plus pertinents pour répondre à la demande utilisateur. Cette demande utilisateur et le résultat de la recherche sémantique seront concaténer dans un prompt qui sera ensuite envoyé au LLM pour obtenir une réponse.


## 3. Q&A 
What is Q&A ?     

source : https://huggingface.co/tasks/question-answering
Le Q/A consiste à fournir une réponse à une question posée en langage naturel.
3 variantes : extractive Q/A, open generative Q/A et close generative Q/A
L'extractive Q/A est le plus adapté à notre projet et permet de fournir une réponse basée sur un document ou une base de connaissances. 

### Comparaison QA Traditionnel vs RAG

source : [paywall] https://medium.com/@tselvaraj/rag-vs-traditional-question-answering-systems-a-paradigm-shift-in-ai-cc771af17fe4

| Critère | QA Traditionnel | RAG |
| ---------------------- | -------------------------------------- | -------------------------------------------- |
| **Source BDD/KB**      | Base de données fixe                   | Base de connaissances dynamique et évolutive |
| **Génération réponse** | Basée sur l'extraction                 | Génération basée sur le contexte récupéré    |
| **Complexité**         | Limitée aux modèles prédéfinis         | Gère les requêtes nuancées et complexes      |
| **Adaptabilité**       | Nécessite des mises à jour système     | Intègre facilement de nouvelles informations |
| **Explicabilité**      | Souvent limité à la citation de source | Peut fournir raisonnement et explications    |
| ---------------------- | -------------------------------------- | -------------------------------------------- |

## 4. API Inférence de HuggingFace

source : https://huggingface.co/docs/inference-providers/index
Ce service offre une interface unifiée permettant d'accéder aux modèles d'IA hébergés par divers fournisseurs d'infrastructure (comme Cerebras, Cohere, Groq, Scaleway, etc.) via une API unique et cohérente.
Avantages : - On peut basculer d'un fournisseur à un autre sans modifier le code.
            - Une seule API pour utiliser differents type d'IA (LLM, VLM, S2T, Feature extraction,...)
            - Cela à un coût mais suffisant pour développer une POC.

De nombreuses autres ressources permmettent sont disponibles depuis cette page tels que des docs d'intégrations, les tâches, les fournisseurs, le hub d'intégration avec l'écosytème HF.

## 05. Sentence-transformers (embeddings)

source : Gemini
Il existe de nombreux modèles de sentence-transformers disponibles sur HuggingFace mais aussi d'autres alternatives open-source tel que HuugingFace (SentenceTransformers) BAAI (Beijing Academy of AI et leurs modèles BGE), Alibaba (Qwen embeddings, General Text Embeddings), Google(EmbeddingGemma, Gecko), Nomic AI(Nomic Embed). On ne citera pas les modèles payant car ils sont hors périmètre.  
On se concentrera sur Sentence Transformers d'Hugging Face car il est open source et gratuit. Il permet de convertir du texte en vecteurs (embeddings) car il comprends le contexte de la phrase, est optimisé pour la recherche sémemtique et pour sa facilité d'usage.

GitHub Sentence Transformer : https://github.com/huggingface/sentence-transformers

## 06. Fastapi

peas encore vu.
